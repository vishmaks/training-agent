import streamlit as st
from langchain_openai import OpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_community.embeddings import HuggingFaceEmbeddings

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("ü§ñ –¢—Ä–µ–Ω–∏–Ω–≥-–∞–≥–µ–Ω—Ç")
st.write("–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∫–µ–π—Å–∞!")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
@st.cache_resource
def load_vector_db():
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectordb = Chroma(persist_directory="chroma_db", embedding_function=embeddings)
    return vectordb

vectordb = load_vector_db()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM
llm = OpenAI(temperature=0)

# –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫—É –¥–ª—è QA
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True
)

# –í–≤–æ–¥ –≤–æ–ø—Ä–æ—Å–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
question = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:")

if question:
    with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
        response = qa_chain.invoke({
            "query": f"–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–µ–π—Å–∞. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç, —Å–∫–∞–∂–∏ '–í –∫–µ–π—Å–µ —ç—Ç–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ'. –í–æ–ø—Ä–æ—Å: {question}"
        })

        answer = response["result"]
        sources = response.get("source_documents", [])

        st.subheader("–û—Ç–≤–µ—Ç")
        st.write(answer)

        if sources:
            st.subheader("–ò—Å—Ç–æ—á–Ω–∏–∫–∏")
            for doc in sources:
                st.write(f"- {doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
